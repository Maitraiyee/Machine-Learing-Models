{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "decision trees are about testing an attribute and branching the cases, based on the result of the test. Each internal node corresponds to a test. And each branch corresponds to a result of the test. And each leaf node assigns a patient to a class.\n",
    "\n",
    "A decision tree can be constructed by considering the attributes one by one.\n",
    "1) Choose an attribute from our dataset.\n",
    "2) Calculate the significance of the attribute in the splitting of the data.\n",
    "3) split the data based on the value of the best attribute.\n",
    "4) go to each branch and repeat it for the rest of the attributes\n",
    "\n",
    "**best attribute: more predictiveness, lower entropy and less impurity**\n",
    "A node in the tree is considered “pure” if, in 100% of the cases, the nodes fall into a specific category of the target field.\n",
    "Entropy is the amount of information disorder, or the amount of randomness in the data.The entropy is used to calculate the homogeneity of the samples in that node. If the samples are completely homogeneous the entropy is zero and if the samples are equally divided, it has an entropy of one.\n",
    "\n",
    "**Entropy = -p(A)log(p(A)) - p(B)(log(p(B)))** where p is the proportion of the category\n",
    "Choose the independent variable then calculate the entropy after split for target variable: eg say target var is drugA and DrugB, independent var is sex, for male- entropy of the target var, for female- entropy of the target var then calculate information gain. \n",
    "\n",
    "**Choose the independent var with higher information gain after split**\n",
    "Information gain is the information that can increase the level of certainty after splitting. **It is the entropy of a tree before the split minus the weighted entropy after the split by an attribute**\n",
    "\n",
    "As entropy, or the amount of randomness, decreases, the information gain, or amount of certainty, increases, and vice-versa.\n",
    "We will consider the entropy over the distribution of samples falling under each leaf node, and we’ll take a weighted average of that entropy – weighted by the proportion of samples falling under that leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Sex</th>\n",
       "      <th>BP</th>\n",
       "      <th>Cholesterol</th>\n",
       "      <th>Na_to_K</th>\n",
       "      <th>Drug</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>F</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>25.355</td>\n",
       "      <td>drugY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>13.093</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>M</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>10.114</td>\n",
       "      <td>drugC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>F</td>\n",
       "      <td>NORMAL</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>7.798</td>\n",
       "      <td>drugX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>F</td>\n",
       "      <td>LOW</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>18.043</td>\n",
       "      <td>drugY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Age Sex      BP Cholesterol  Na_to_K   Drug\n",
       "0   23   F    HIGH        HIGH   25.355  drugY\n",
       "1   47   M     LOW        HIGH   13.093  drugC\n",
       "2   47   M     LOW        HIGH   10.114  drugC\n",
       "3   28   F  NORMAL        HIGH    7.798  drugX\n",
       "4   61   F     LOW        HIGH   18.043  drugY"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## reading the dataset \n",
    "df = pd.read_csv('drug200.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "drugY    91\n",
       "drugX    54\n",
       "drugA    23\n",
       "drugC    16\n",
       "drugB    16\n",
       "Name: Drug, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## selecting independent vars to array x \n",
    "x = df.iloc[:,:-1].values\n",
    "\n",
    "##checking distribution for target variable\n",
    "y = df.iloc[:,-1].values\n",
    "df.iloc[:,-1].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some features in this dataset are categorical such as **Sex** or **BP**. Unfortunately, Sklearn Decision Trees do not handle categorical variables. But still we can convert these features to numerical values. **pandas.get_dummies()**\n",
    "Convert categorical variable into dummy/indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23, 0, 0, 0, 25.355],\n",
       "       [47, 1, 1, 0, 13.093],\n",
       "       [47, 1, 1, 0, 10.113999999999999],\n",
       "       [28, 0, 2, 0, 7.797999999999999],\n",
       "       [61, 0, 1, 0, 18.043]], dtype=object)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LabelEncoder: ordered set: incremental value, OneHotEncoder: unordered set \n",
    "from sklearn import preprocessing\n",
    "n_sex = preprocessing.LabelEncoder()\n",
    "n_sex.fit(['F','M'])\n",
    "x[:,1] = n_sex.transform(x[:,1])\n",
    "\n",
    "n_bp = preprocessing.LabelEncoder()\n",
    "n_bp.fit(['LOW', 'NORMAL', 'HIGH'])\n",
    "x[:,2] = n_bp.transform(x[:,2])\n",
    "\n",
    "n_chol = preprocessing.LabelEncoder()\n",
    "n_chol.fit(['NORMAL', 'HIGH'])\n",
    "x[:,3] = n_chol.transform(x[:,3])\n",
    "\n",
    "x[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now <b> train_test_split </b> will return 4 different parameters. We will name them:<br>\n",
    "X_trainset, X_testset, y_trainset, y_testset <br> <br>\n",
    "The <b> train_test_split </b> will need the parameters: <br>\n",
    "X, y, test_size=0.3, and random_state=3. <br> <br>\n",
    "The <b>X</b> and <b>y</b> are the arrays required before the split, the <b>test_size</b> represents the ratio of the testing dataset, and the <b>random_state</b> ensures that we obtain the same splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(140, 5)\n",
      "(140,)\n",
      "(60, 5)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "## train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 3)\n",
    "\n",
    "#checking train set dimensions \n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "#checking test set dimensions\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=4, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## creating instance drugtree with criterion as entropy \n",
    "drugtree = DecisionTreeClassifier(criterion = 'entropy', max_depth = 4)\n",
    "drugtree #shows all default parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drugY' 'drugX' 'drugX' 'drugX' 'drugX']\n",
      "['drugY' 'drugX' 'drugX' 'drugX' 'drugX']\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "drugtree.fit(x_train, y_train)\n",
    "yhat = drugtree.predict(x_test)\n",
    "\n",
    "# printing prediction \n",
    "print(yhat[0:5])\n",
    "print(y_test[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree's Accuracy:  0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(\"Decision Tree's Accuracy: \", metrics.accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
