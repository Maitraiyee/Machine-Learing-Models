{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "Logistic regression, we predict a variable which is binary, such as, Yes/No, TRUE/FALSE, successful or Not successful, pregnant/Not pregnant, and so on, all of which can all be coded as 0 or 1. In logistic regression, dependent variables should be continuous; if categorical, they should be dummy or indicator-coded. This means we have to transform them to some continuous value.\n",
    "not only do we predict the class of each case, we also measure the probability of a case belonging to a specific class.\n",
    "\n",
    "Here are four situations in which Logistic regression is a good candidate:\n",
    "1) When the target field in your data is categorical, or specifically, is binary, such as 0/1, yes/no, churn or no churn, positive/negative, and so on.\n",
    "2) You need the probability of your prediction, for example, if you want to know what the probability is, of a customer buying a product. Logistic regression returns a probability score between 0 and 1 for a given sample of data. logistic regressing predicts the probability of that sample, and we map the cases to a discrete class based on that probability.\n",
    "3) If your data is linearly seperable.The decision boundary of logistic regression is a line or a plane or a hyper-plane.\n",
    "4) You need to understand the impact of a feature. You can select the best features based on the statistical significance of the logistic regression model coefficients or parameters.\n",
    "\n",
    "function given by sigmoid(theta transpose x). \n",
    "Sigmoid function: Instead of calculating the value of Œ∏^T X, directly, it returns the probability that a Œ∏^T X is very big or very small.\n",
    "In logistic regression, we model the probability that an input (X) belongs to the default class (Y=1), and we can write this formally as, P(Y=1|X). We can also write P(y=0|X) = 1 -P(y=1|x). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To find optimal theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1) Initialize ùúÉ vector with random values, as with most machine learning algorithms, for example ‚àí1 or 2.\n",
    "Step 2) Calculate the model output, which is œÉ(Œ∏^T X), for a sample customer in your training set.\n",
    "Step 3) Compare the output of our model, y^, which could be a value of, let‚Äôs say, 0.7, with the actual label of the customer,         which is for example, 1 for churn.\n",
    "Step 4) Calculate the error for all customers as we did in the previous steps, and add up these errors. The total error is the         cost of your model, and is calculated by the model‚Äôs cost function.\n",
    "**The lower the cost, the better the model is at estimating the customer‚Äôs labels correctly**\n",
    "Step 5) Change the value of theta and repeat from step 2 to reach to optimal value \n",
    "change theta: gradient descent, when to stop? calculate accuracy and stop at a satisfacory value\n",
    "\n",
    "**cost function : mean squared error**\n",
    "**for log regression we use: Log Loss= -1/n[y(1-log(yhat)+(1-y)log(1-yhat))] **\n",
    "\n",
    "### Gradient Descent\n",
    "gradient descent is an iterative approach to finding the minimum of a function. The gradient is the slope of the surface at every point. And, the direction of the gradient is the direction of the greatest uphill. if we calculate the derivative\n",
    "of J with respect to ùúÉ1, we find out that it is a positive number. This indicates that function is increasing as ùúÉ1 increases. So, to decrease J, we should move in the opposite direction.This means to move in the direction of the negative derivative for ùúÉ1, i.e. slope.\n",
    "\n",
    "The gradient value also indicates how big of a step to take.\n",
    "If the slope is large, we should take a large step because we‚Äôre far from the minimum.\n",
    "If the slope is small we should to take a smaller step.\n",
    "Gradient descent takes increasingly smaller steps towards the minimum with each iteration.\n",
    "\n",
    "The equation returns the slope of that point, and we should update the parameter in the opposite direction of the slope.\n",
    "\n",
    "**newtheta = oldtheta - mu*gradient**\n",
    "where mu is the learning rate \n",
    "\n",
    "Learning rate gives us additional control on how fast we move on the surface.\n",
    "In sum, we can simply say, Gradient descent is like taking steps in the current direction\n",
    "of the slope, and the learning rate is like the length of the step you take.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "\n",
    "## What is the difference between Linear and Logistic Regression?\n",
    "\n",
    "While Linear Regression is suited for estimating continuous values (e.g. estimating house price), it is not the best tool for predicting the class of an observed data point. In order to estimate the class of a data point, we need some sort of guidance on what would be the <b>most probable class</b> for that data point. For this, we use <b>Logistic Regression</b>.\n",
    "\n",
    "<div class=\"alert alert-success alertsuccess\" style=\"margin-top: 20px\">\n",
    "<font size = 3><strong>Recall linear regression:</strong></font>\n",
    "<br>\n",
    "<br>\n",
    "    As you know, <b>Linear regression</b> finds a function that relates a continuous dependent variable, <b>y</b>, to some predictors (independent variables $x_1$, $x_2$, etc.). For example, Simple linear regression assumes a function of the form:\n",
    "<br><br>\n",
    "$$\n",
    "y = \\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 + \\cdots\n",
    "$$\n",
    "<br>\n",
    "and finds the values of parameters $\\theta_0, \\theta_1, \\theta_2$, etc, where the term $\\theta_0$ is the \"intercept\". It can be generally shown as:\n",
    "<br><br>\n",
    "$$\n",
    "‚Ñé_\\theta(ùë•) = \\theta^TX\n",
    "$$\n",
    "<p></p>\n",
    "\n",
    "</div>\n",
    "\n",
    "Logistic Regression is a variation of Linear Regression, useful when the observed dependent variable, <b>y</b>, is categorical. It produces a formula that predicts the probability of the class label as a function of the independent variables.\n",
    "\n",
    "Logistic regression fits a special s-shaped curve by taking the linear regression and transforming the numeric estimate into a probability with the following function, which is called sigmoid function ùúé:\n",
    "\n",
    "$$\n",
    "‚Ñé_\\theta(ùë•) = \\sigma({\\theta^TX}) =  \\frac {e^{(\\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 +...)}}{1 + e^{(\\theta_0 + \\theta_1  x_1 + \\theta_2  x_2 +\\cdots)}}\n",
    "$$\n",
    "Or:\n",
    "$$\n",
    "ProbabilityOfaClass_1 =  P(Y=1|X) = \\sigma({\\theta^TX}) = \\frac{e^{\\theta^TX}}{1+e^{\\theta^TX}} \n",
    "$$\n",
    "\n",
    "In this equation, ${\\theta^TX}$ is the regression result (the sum of the variables weighted by the coefficients), `exp` is the exponential function and $\\sigma(\\theta^TX)$ is the sigmoid or [logistic function](http://en.wikipedia.org/wiki/Logistic_function?cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0101EN-Coursera-20231514&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0101EN-Coursera-20231514&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0101EN-Coursera-20231514&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ&cm_mmc=Email_Newsletter-_-Developer_Ed%2BTech-_-WW_WW-_-SkillsNetwork-Courses-IBMDeveloperSkillsNetwork-ML0101EN-Coursera-20231514&cm_mmca1=000026UJ&cm_mmca2=10006555&cm_mmca3=M12345678&cvosrc=email.Newsletter.M12345678&cvo_campaign=000026UJ), also called logistic curve. It is a common \"S\" shape (sigmoid curve).\n",
    "\n",
    "So, briefly, Logistic Regression passes the input through the logistic/sigmoid but then treats the result as a probability:\n",
    "\n",
    "<img\n",
    "src=\"https://ibm.box.com/shared/static/kgv9alcghmjcv97op4d6onkyxevk23b1.png\" width=\"400\" align=\"center\">\n",
    "\n",
    "The objective of **Logistic Regression** algorithm, is to find the best parameters Œ∏, for $‚Ñé_\\theta(ùë•)$ = $\\sigma({\\theta^TX})$, in such a way that the model best predicts the class of each case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer churn with logistic regression \n",
    "A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##importing the libraries \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "import scipy.optimize as opt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set provides information to help you predict what behavior will help you to retain customers. You can analyze all relevant customer data and develop focused customer retention programs.\n",
    "\n",
    "The dataset includes information about:\n",
    "\n",
    "-   Customers who left within the last month ‚Äì the column is called Churn\n",
    "-   Services that each customer has signed up for ‚Äì phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
    "-   Customer account information ‚Äì how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
    "-   Demographic info about customers ‚Äì gender, age range, and if they have partners and dependents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>...</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482</td>\n",
       "      <td>3.033</td>\n",
       "      <td>4.913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.841</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>4.331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.091</td>\n",
       "      <td>4.382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "\n",
       "   longmon  ...  pager  internet  callwait  confer  ebill  loglong  logtoll  \\\n",
       "0     4.40  ...    1.0       0.0       1.0     1.0    0.0    1.482    3.033   \n",
       "1     9.45  ...    0.0       0.0       0.0     0.0    0.0    2.246    3.240   \n",
       "2     6.30  ...    0.0       0.0       0.0     1.0    0.0    1.841    3.240   \n",
       "3     6.05  ...    1.0       1.0       1.0     1.0    1.0    1.800    3.807   \n",
       "4     7.10  ...    0.0       0.0       1.0     1.0    0.0    1.960    3.091   \n",
       "\n",
       "   lninc  custcat  churn  \n",
       "0  4.913      4.0    1.0  \n",
       "1  3.497      1.0    1.0  \n",
       "2  3.401      3.0    0.0  \n",
       "3  4.331      4.0    0.0  \n",
       "4  4.382      3.0    0.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('ChurnData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "\n",
       "   churn  \n",
       "0      1  \n",
       "1      1  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## selecting the relevant features into the df \n",
    "##changing the datatype of the dependent variable to int as required by logistic regression algo \n",
    "df = df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "df['churn'] = df['churn'].astype('int')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 11.  33.   7. 136.   5.   5.   0.   1.   1.]\n",
      " [ 33.  33.  12.  33.   2.   0.   0.   0.   0.]\n",
      " [ 23.  30.   9.  30.   1.   2.   0.   0.   0.]\n",
      " [ 38.  35.   5.  76.   2.  10.   1.   1.   1.]\n",
      " [  7.  35.  14.  80.   2.  15.   0.   1.   0.]]\n",
      "[1 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "x = df.iloc[:,:-1].values\n",
    "y = df.iloc[:,-1].values\n",
    "print(x[0:5])\n",
    "print(y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.13518441, -0.62595491, -0.4588971 ,  0.4751423 ,  1.6961288 ,\n",
       "        -0.58477841, -0.85972695,  0.64686916,  1.56469673],\n",
       "       [-0.11604313, -0.62595491,  0.03454064, -0.32886061, -0.6433592 ,\n",
       "        -1.14437497, -0.85972695, -1.54590766, -0.63910148],\n",
       "       [-0.57928917, -0.85594447, -0.261522  , -0.35227817, -1.42318853,\n",
       "        -0.92053635, -0.85972695, -1.54590766, -0.63910148],\n",
       "       [ 0.11557989, -0.47262854, -0.65627219,  0.00679109, -0.6433592 ,\n",
       "        -0.02518185,  1.16316   ,  0.64686916,  1.56469673],\n",
       "       [-1.32048283, -0.47262854,  0.23191574,  0.03801451, -0.6433592 ,\n",
       "         0.53441472, -0.85972695,  0.64686916, -0.63910148]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scaling the data \n",
    "from sklearn import preprocessing \n",
    "x = preprocessing.StandardScaler().fit(x).transform(x)\n",
    "x[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (40, 9) (40,)\n",
      "Test set: (160, 9) (160,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = 0.2, random_state = 4)\n",
    "print ('Train set:', x_train.shape,  y_train.shape)\n",
    "print ('Test set:', x_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building \n",
    "model using **LogisticRegression** from Scikit-learn package. This function implements logistic regression and can use different numerical optimizers to find parameters, including ‚Äònewton-cg‚Äô, ‚Äòlbfgs‚Äô, ‚Äòliblinear‚Äô, ‚Äòsag‚Äô, ‚Äòsaga‚Äô solvers.\n",
    "The version of Logistic Regression in Scikit-learn, support regularization. Regularization is a technique used to solve the overfitting problem in machine learning models.\n",
    "\n",
    "**C** parameter indicates **inverse of regularization strength** which must be a positive float. Smaller values specify stronger regularization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "LR = LogisticRegression(C = 0.01, solver = 'liblinear').fit(x_train,y_train)\n",
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = LR.predict(x_test)\n",
    "yhat[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probability prediction : \n",
    "**predict_proba**  returns estimates for all classes, ordered by the label of classes. So, the first column is the probability of class 1, P(Y=1|X), and second column is probability of class 0, P(Y=0|X):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51237594, 0.48762406],\n",
       "       [0.52592756, 0.47407244],\n",
       "       [0.52669527, 0.47330473],\n",
       "       [0.5656521 , 0.4343479 ],\n",
       "       [0.51955519, 0.48044481]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_prob = LR.predict_proba(x_test)\n",
    "yhat_prob[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gauta\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:664: FutureWarning: jaccard_similarity_score has been deprecated and replaced with jaccard_score. It will be removed in version 0.23. This implementation has surprising behavior for binary and multiclass classification tasks.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.70625"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "jaccard_similarity_score(y_test, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[30 17]\n",
      " [30 83]]\n"
     ]
    }
   ],
   "source": [
    "## confusion matrix \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# to print confusion matrix     \n",
    "print(confusion_matrix(y_test, yhat, labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix, without normalization\n",
      "[[30 17]\n",
      " [30 83]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, yhat, labels=[1,0])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['churn=1','churn=0'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.73      0.78       113\n",
      "           1       0.50      0.64      0.56        47\n",
      "\n",
      "    accuracy                           0.71       160\n",
      "   macro avg       0.67      0.69      0.67       160\n",
      "weighted avg       0.73      0.71      0.72       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification_report to see recall precision and f1-score \n",
    "print (classification_report(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the count of each section, we can calculate precision and recall of each label:\n",
    "\n",
    "-   **Precision** is a measure of the accuracy provided that a class label has been predicted. It is defined by: precision = TP¬†/¬†(TP¬†+¬†FP)\n",
    "\n",
    "-   **Recall** is true positive rate. It is defined as: Recall = ¬†TP¬†/¬†(TP¬†+¬†FN)\n",
    "\n",
    "So, we can calculate precision and recall of each class.\n",
    "\n",
    "**F1 score:**\n",
    "Now we are in the position to calculate the F1 scores for each label based on the precision and recall of that label. \n",
    "\n",
    "The F1 score is the harmonic average of the¬†precision and recall, where an F1¬†score reaches its best value at 1 (perfect precision and recall) and worst at 0. It is a good way to show that a classifer has a good value for both recall and precision.\n",
    "\n",
    "And finally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 0.72 in our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6379880743775812"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log_loss is measured with yhat_prob\n",
    "from sklearn.metrics import log_loss\n",
    "log_loss(y_test, yhat_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
